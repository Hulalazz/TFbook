{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wjj/TFbook/chapter5/models-master/tutorials/rnn/ptb\n"
     ]
    }
   ],
   "source": [
    "cd ../chapter5/models-master/tutorials/rnn/ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*- \n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBInput(object):\n",
    "    \n",
    "    def __init__(self,config,data,name=None):\n",
    "        self.batch_size = batch_size = config.batch_size    #从config中读取参数存到本地变量\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        self.epoch_size = (len(data) // batch_size - 1) // num_steps\n",
    "        self.input_data, self.targets = reader.ptb_producer(data,batch_size,num_steps,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "    def __init__(self,is_training,config,input_):\n",
    "        self._input = input_\n",
    "        \n",
    "        batch_size = input_.batch_size    #从input_中读取参数存到本地变量\n",
    "        num_steps = input_.num_steps\n",
    "        size = config.hidden_size    #从config中读取参数存到本地变量，隐含节点个数\n",
    "        vocab_size = config.vocab_size\n",
    "        def lstm_cell():    #使用tf.contrib.rnn.BasicLSTMCell函数设置默认的LSTM单元\n",
    "            return tf.contrib.rnn.BasicLSTMCell(size,forget_bias=0.0,state_is_tuple=True)    #size是隐含节点个数，forgets_bias是forget gate的bias，\n",
    "        attn_cell = lstm_cell\n",
    "        if is_training and config.keep_prob < 1:    #如果是在训练状态且keepprob<1则在前面的lstm_cell之后接一个Dropout层\n",
    "            def attn_cell():    #使用tf.contrib.rnn.DropoutWrapper函数设置一个dropout层\n",
    "                return tf.contrib.rnn.DropoutWrapper(lstm_cell(),output_keep_prob=config.keep_prob)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(config.num_layers)],state_is_tuple=True)#用tf.contrib.rnn.MultiRNNCell函数堆叠前面构造的lstm_cell\n",
    "\n",
    "        self._initial_state = cell.zero_state(batch_size, tf.float32)    #设置LSTM单元的初始化状态为0    \n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            embedding = tf.get_variable(      #embedding是一个向量， 将one-hot编码格式的单词转化为向量表达形式\n",
    "                \"embedding\", [vocab_size,size],dtype=tf.float32)    #vocab_size是词汇表数，每个单词向量表达所需的维数为size   分别构成embedding的行和列\n",
    "            inputs = tf.nn.embedding_lookup(embedding,input_.input_data)    #查询单词对应的向量表达获得inputs\n",
    "        if is_training and config.keep_prob <1:    #如果是训练状态，还要在后面加上一层dropout层\n",
    "            inputs = tf.nn.dropout(inputs,config.keep_prob)\n",
    "\n",
    "        outputs = []\n",
    "        state = self._initial_state\n",
    "        with tf.variable_scope(\"RNN\"):     #将下面的操作设为RNN\n",
    "            for time_step in range(num_steps):    #设置步数，用来限制梯度在反向传播过程步数\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()    #第二次循环开始设置复用变量\n",
    "                (cell_output,state) = cell(inputs[:,time_step,:],state) #inputs的三个维度，第1个代表batch中的第几个样本，第2个代表样本中的第几个单词，第三个代表单词的\n",
    "                outputs.append(cell_output)\n",
    "\n",
    "        output = tf.reshape(tf.concat(outputs,1),[-1,size]) \n",
    "        softmax_w = tf.get_variable(\n",
    "                    \"softmax_w\",[size,vocab_size],dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\",[vocab_size],dtype=tf.float32)\n",
    "        logits = tf.matmul(output,softmax_w) + softmax_b\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(   #用这个函数来计算targets和logits的偏差\n",
    "                [logits],\n",
    "                [tf.reshape(input_.targets,[-1])],\n",
    "                [tf.ones([batch_size * num_steps],dtype=tf.float32)])\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size    #汇总batch的误差，在计算每个样本的误差\n",
    "        self._final_state = state    #保留最终的状态\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "        self._lr = tf.Variable(0.0,trainable=False)   #定义学习率。且设置为不可训练\n",
    "        tvars = tf.trainable_variables()    \n",
    "        grads,_ = tf.clip_by_global_norm(tf.gradients(cost,tvars),  #计算tvars的梯度，设置梯度的最大范数,\n",
    "                                        config.max_grad_norm)    #这个Gradient Cliping方法，控制梯度的最大范数，某种程度上有正则化的效果，防止梯度爆炸问题\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads,tvars),    #定义一个训练操作，将clip过的梯度应用到所有了训练的参数上\n",
    "                        global_step=tf.contrib.framework.get_or_create_global_step())    #生成全局统一的训练步数\n",
    "\n",
    "        self._new_lr = tf.placeholder(\n",
    "                    tf.float32,shape=[],name=\"new_learning_rate\")   #控制学习速率\n",
    "        self._lr_update = tf.assign(self._lr,self._new_lr)  #将新的学习速率赋值给当前的学习速率_lr\n",
    "        \n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(self._lr_update,feed_dict={self._new_lr:lr_value})\n",
    "    \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "    \n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "    \n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "    \n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "    \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "    init_scale = 0.1\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 20\n",
    "    hidden_size = 200\n",
    "    max_epoch = 4 \n",
    "    max_max_epoch = 13\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MediumConfig(object):\n",
    "    init_scale = 0.05\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 5\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 650\n",
    "    max_epoch = 6\n",
    "    max_max_epoch = 39\n",
    "    keep_prob = 0.5\n",
    "    lr_decay = 0.8\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LargeConfig(object):\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 10\n",
    "    num_layers = 2\n",
    "    num_steps = 35\n",
    "    hidden_size = 1500\n",
    "    max_epoch = 14\n",
    "    max_max_epoch = 55\n",
    "    keep_prob = 0.35\n",
    "    lr_decay = 1 / 1.15\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestConfig(object):\n",
    "    init_scale = 0.04\n",
    "    learning_rate = 1.0\n",
    "    max_grad_norm = 1\n",
    "    num_layers = 1\n",
    "    num_steps = 2\n",
    "    hidden_size = 2\n",
    "    max_epoch = 1\n",
    "    max_max_epoch = 1\n",
    "    keep_prob = 1.0\n",
    "    lr_decay = 0.5\n",
    "    batch_size = 20\n",
    "    vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(session,model,eval_op=None,verbose=False):\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)    #初始化获得初始状态\n",
    "    \n",
    "    fetches = {\n",
    "        \"cost\":model.cost,\n",
    "        \"final_state\":model.final_state,\n",
    "    }\n",
    "    if eval_op is not None:\n",
    "        fetches[\"eval_op\"] = eval_op    #创建结果的字典表\n",
    "    \n",
    "    for step in range(model.input.epoch_size):    #训练epoch_size\n",
    "        feed_dict = {}\n",
    "        for i,(c,h) in enumerate(model.initial_state):    #每次把state装入feed_dict\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "        vals = session.run(fetches,feed_dict)   #跑起\n",
    "        cost = vals[\"cost\"]     #得到cost\n",
    "        state = vals[\"final_state\"]   #得到state\n",
    "        \n",
    "        costs += costs    #累加cost\n",
    "        iters += model.input.num_steps   #累加迭代次数，\n",
    "        \n",
    "        if verbose and step % (model.input.epoch_size // 10) == 10:    #每隔10次做一次展示\n",
    "            print(\"%.3f perplexity:%.3f speed: %.0f wps\" %\n",
    "                 (step * 1.0 / model.input.epoch_size,np.exp(costs/iters),\n",
    "                 iters * model.input.batch_size / (time.time()-start_time)))\n",
    "                  \n",
    "    return np.exp(costs / iters)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path='/home/wjj/TFbook/chapter7/simple-examples/data/'\n",
    "raw_data = reader.ptb_raw_data(data_path)     #直接读取解压后的数据\n",
    "train_data,valid_data,test_data,_ = raw_data    #将解压后的数据分别存为训练数据和验证数据以及测试数据\n",
    "config = SmallConfig()    #使用SmallConfig的配置\n",
    "eval_config = SmallConfig()  #测试配置eval_config需和训练配置一致\n",
    "eval_config.batch_size = 1\n",
    "eval_config.num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102,\n",
       " 14,\n",
       " 24,\n",
       " 32,\n",
       " 752,\n",
       " 381,\n",
       " 2,\n",
       " 29,\n",
       " 120,\n",
       " 0,\n",
       " 35,\n",
       " 92,\n",
       " 60,\n",
       " 111,\n",
       " 143,\n",
       " 32,\n",
       " 616,\n",
       " 3148,\n",
       " 282,\n",
       " 19,\n",
       " 0,\n",
       " 447,\n",
       " 459,\n",
       " 438,\n",
       " 196,\n",
       " 1621,\n",
       " 3,\n",
       " 394,\n",
       " 90,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 0,\n",
       " 1106,\n",
       " 1471,\n",
       " 14,\n",
       " 3152,\n",
       " 1858,\n",
       " 5,\n",
       " 1337,\n",
       " 39,\n",
       " 1079,\n",
       " 4,\n",
       " 6803,\n",
       " 2,\n",
       " 57,\n",
       " 2162,\n",
       " 4857,\n",
       " 3845,\n",
       " 78,\n",
       " 0,\n",
       " 522,\n",
       " 3,\n",
       " 1037,\n",
       " 779,\n",
       " 51,\n",
       " 74,\n",
       " 901,\n",
       " 280,\n",
       " 117,\n",
       " 2283,\n",
       " 5,\n",
       " 4102,\n",
       " 0,\n",
       " 399,\n",
       " 3866,\n",
       " 7,\n",
       " 179,\n",
       " 149,\n",
       " 8,\n",
       " 288,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 60,\n",
       " 2569,\n",
       " 365,\n",
       " 16,\n",
       " 0,\n",
       " 129,\n",
       " 146,\n",
       " 1022,\n",
       " 0,\n",
       " 838,\n",
       " 8,\n",
       " 2888,\n",
       " 4,\n",
       " 69,\n",
       " 3222,\n",
       " 56,\n",
       " 46,\n",
       " 3000,\n",
       " 78,\n",
       " 0,\n",
       " 3,\n",
       " 1037,\n",
       " 496,\n",
       " 554,\n",
       " 79,\n",
       " 32,\n",
       " 2444,\n",
       " 0,\n",
       " 399,\n",
       " 844,\n",
       " 2,\n",
       " 129,\n",
       " 145,\n",
       " 247,\n",
       " 2066,\n",
       " 5,\n",
       " 1213,\n",
       " 52,\n",
       " 5,\n",
       " 0,\n",
       " 5491,\n",
       " 5,\n",
       " 414,\n",
       " 0,\n",
       " 8654,\n",
       " 1022,\n",
       " 280,\n",
       " 17,\n",
       " 360,\n",
       " 129,\n",
       " 4234,\n",
       " 4,\n",
       " 60,\n",
       " 280,\n",
       " 117,\n",
       " 2,\n",
       " 772,\n",
       " 399,\n",
       " 4,\n",
       " 739,\n",
       " 82,\n",
       " 890,\n",
       " 9,\n",
       " 3919,\n",
       " 216,\n",
       " 288,\n",
       " 7,\n",
       " 482,\n",
       " 1,\n",
       " 2770,\n",
       " 149,\n",
       " 3006,\n",
       " 2,\n",
       " 914,\n",
       " 129,\n",
       " 146,\n",
       " 149,\n",
       " 408,\n",
       " 2981,\n",
       " 3564,\n",
       " 6705,\n",
       " 3953,\n",
       " 180,\n",
       " 7930,\n",
       " 1684,\n",
       " 1973,\n",
       " 8,\n",
       " 691,\n",
       " 5852,\n",
       " 96,\n",
       " 1898,\n",
       " 77,\n",
       " 8,\n",
       " 576,\n",
       " 5807,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 30,\n",
       " 292,\n",
       " 2334,\n",
       " 2,\n",
       " 0,\n",
       " 621,\n",
       " 47,\n",
       " 24,\n",
       " 1,\n",
       " 2,\n",
       " 496,\n",
       " 554,\n",
       " 0,\n",
       " 2198,\n",
       " 46,\n",
       " 63,\n",
       " 583,\n",
       " 5,\n",
       " 2444,\n",
       " 0,\n",
       " 6470,\n",
       " 16,\n",
       " 0,\n",
       " 1022,\n",
       " 4,\n",
       " 0,\n",
       " 35,\n",
       " 92,\n",
       " 60,\n",
       " 111,\n",
       " 15,\n",
       " 2994,\n",
       " 1,\n",
       " 444,\n",
       " 232,\n",
       " 70,\n",
       " 18,\n",
       " 1,\n",
       " 125,\n",
       " 584,\n",
       " 2,\n",
       " 1,\n",
       " 557,\n",
       " 1,\n",
       " 141,\n",
       " 4,\n",
       " 2198,\n",
       " 5415,\n",
       " 997,\n",
       " 80,\n",
       " 14,\n",
       " 13,\n",
       " 1547,\n",
       " 5,\n",
       " 117,\n",
       " 0,\n",
       " 2569,\n",
       " 13,\n",
       " 32,\n",
       " 839,\n",
       " 49,\n",
       " 789,\n",
       " 2,\n",
       " 67,\n",
       " 0,\n",
       " 332,\n",
       " 13,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 113,\n",
       " 876,\n",
       " 247,\n",
       " 562,\n",
       " 32,\n",
       " 1154,\n",
       " 14,\n",
       " 2,\n",
       " 3228,\n",
       " 26,\n",
       " 2374,\n",
       " 11,\n",
       " 6,\n",
       " 3465,\n",
       " 4,\n",
       " 1856,\n",
       " 10,\n",
       " 13,\n",
       " 63,\n",
       " 83,\n",
       " 7,\n",
       " 0,\n",
       " 47,\n",
       " 2,\n",
       " 97,\n",
       " 161,\n",
       " 586,\n",
       " 8,\n",
       " 57,\n",
       " 280,\n",
       " 50,\n",
       " 292,\n",
       " 696,\n",
       " 51,\n",
       " 1150,\n",
       " 266,\n",
       " 282,\n",
       " 1730,\n",
       " 16,\n",
       " 6,\n",
       " 5044,\n",
       " 6302,\n",
       " 272,\n",
       " 76,\n",
       " 0,\n",
       " 60,\n",
       " 47,\n",
       " 24,\n",
       " 105,\n",
       " 2748,\n",
       " 2,\n",
       " 223,\n",
       " 7,\n",
       " 6,\n",
       " 1,\n",
       " 1227,\n",
       " 0,\n",
       " 447,\n",
       " 459,\n",
       " 2614,\n",
       " 7,\n",
       " 3152,\n",
       " 31,\n",
       " 1471,\n",
       " 8448,\n",
       " 43,\n",
       " 6,\n",
       " 277,\n",
       " 4,\n",
       " 51,\n",
       " 552,\n",
       " 39,\n",
       " 40,\n",
       " 1,\n",
       " 52,\n",
       " 6,\n",
       " 8570,\n",
       " 36,\n",
       " 3,\n",
       " 3,\n",
       " 252,\n",
       " 16,\n",
       " 0,\n",
       " 272,\n",
       " 7,\n",
       " 1,\n",
       " 77,\n",
       " 395,\n",
       " 2,\n",
       " 1,\n",
       " 77,\n",
       " 7240,\n",
       " 5,\n",
       " 3,\n",
       " 21,\n",
       " 71,\n",
       " 6,\n",
       " 439,\n",
       " 11,\n",
       " 0,\n",
       " 129,\n",
       " 146,\n",
       " 2,\n",
       " 18,\n",
       " 0,\n",
       " 235,\n",
       " 4,\n",
       " 0,\n",
       " 272,\n",
       " 3,\n",
       " 21,\n",
       " 71,\n",
       " 46,\n",
       " 926,\n",
       " 2,\n",
       " 0,\n",
       " 447,\n",
       " 459,\n",
       " 2614,\n",
       " 248,\n",
       " 18,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 447,\n",
       " 9,\n",
       " 460,\n",
       " 24,\n",
       " 334,\n",
       " 7,\n",
       " 374,\n",
       " 520,\n",
       " 86,\n",
       " 5,\n",
       " 0,\n",
       " 1,\n",
       " 752,\n",
       " 381,\n",
       " 1037,\n",
       " 10,\n",
       " 2868,\n",
       " 313,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 760,\n",
       " 520,\n",
       " 227,\n",
       " 0,\n",
       " 447,\n",
       " 9,\n",
       " 8018,\n",
       " 24,\n",
       " 0,\n",
       " 1,\n",
       " 821,\n",
       " 8,\n",
       " 0,\n",
       " 9912,\n",
       " 155,\n",
       " 0,\n",
       " 47,\n",
       " 202,\n",
       " 3,\n",
       " 36,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 123,\n",
       " 78,\n",
       " 752,\n",
       " 381,\n",
       " 2,\n",
       " 0,\n",
       " 447,\n",
       " 202,\n",
       " 3,\n",
       " 3,\n",
       " 16,\n",
       " 752,\n",
       " 381,\n",
       " 2,\n",
       " 71,\n",
       " 4,\n",
       " 408,\n",
       " 0,\n",
       " 729,\n",
       " 4,\n",
       " 561,\n",
       " 698,\n",
       " 46,\n",
       " 1965,\n",
       " 1024,\n",
       " 73,\n",
       " 272,\n",
       " 282,\n",
       " 7089,\n",
       " 5,\n",
       " 309,\n",
       " 8,\n",
       " 1918,\n",
       " 43,\n",
       " 0,\n",
       " 497,\n",
       " 12,\n",
       " 3,\n",
       " 48,\n",
       " 588,\n",
       " 4,\n",
       " 0,\n",
       " 763,\n",
       " 17,\n",
       " 31,\n",
       " 1,\n",
       " 96,\n",
       " 2,\n",
       " 330,\n",
       " 321,\n",
       " 9,\n",
       " 7757,\n",
       " 3228,\n",
       " 36,\n",
       " 645,\n",
       " 5903,\n",
       " 50,\n",
       " 1686,\n",
       " 4052,\n",
       " 380,\n",
       " 9608,\n",
       " 10,\n",
       " 6,\n",
       " 420,\n",
       " 42,\n",
       " 2280,\n",
       " 8,\n",
       " 408,\n",
       " 60,\n",
       " 42,\n",
       " 499,\n",
       " 2,\n",
       " 18,\n",
       " 3,\n",
       " 2187,\n",
       " 4686,\n",
       " 544,\n",
       " 0,\n",
       " 1,\n",
       " 309,\n",
       " 0,\n",
       " 129,\n",
       " 146,\n",
       " 24,\n",
       " 1,\n",
       " 77,\n",
       " 7,\n",
       " 408,\n",
       " 1499,\n",
       " 309,\n",
       " 2,\n",
       " 16,\n",
       " 0,\n",
       " 111,\n",
       " 1022,\n",
       " 19,\n",
       " 640,\n",
       " 19,\n",
       " 408,\n",
       " 1898,\n",
       " 77,\n",
       " 64,\n",
       " 1,\n",
       " 11,\n",
       " 6,\n",
       " 3866,\n",
       " 15,\n",
       " 54,\n",
       " 524,\n",
       " 1022,\n",
       " 1281,\n",
       " 2,\n",
       " 249,\n",
       " 280,\n",
       " 79,\n",
       " 25,\n",
       " 919,\n",
       " 7138,\n",
       " 51,\n",
       " 2540,\n",
       " 67,\n",
       " 0,\n",
       " 309,\n",
       " 1,\n",
       " 2,\n",
       " 11,\n",
       " 422,\n",
       " 0,\n",
       " 47,\n",
       " 50,\n",
       " 58,\n",
       " 3106,\n",
       " 43,\n",
       " 3328,\n",
       " 78,\n",
       " 1672,\n",
       " 65,\n",
       " 9,\n",
       " 267,\n",
       " 4662,\n",
       " 6219,\n",
       " 287,\n",
       " 43,\n",
       " 0,\n",
       " 2192,\n",
       " 11,\n",
       " 506,\n",
       " 1327,\n",
       " 1148,\n",
       " 3328,\n",
       " 2,\n",
       " 8,\n",
       " 3,\n",
       " 1495,\n",
       " 78,\n",
       " 0,\n",
       " 408,\n",
       " 77,\n",
       " 2671,\n",
       " 544,\n",
       " 309,\n",
       " 10,\n",
       " 0,\n",
       " 408,\n",
       " 96,\n",
       " 79,\n",
       " 32,\n",
       " 188,\n",
       " 604,\n",
       " 11,\n",
       " 27,\n",
       " 257,\n",
       " 2,\n",
       " 18,\n",
       " 39,\n",
       " 374,\n",
       " 0,\n",
       " 447,\n",
       " 24,\n",
       " 118,\n",
       " 43,\n",
       " 3,\n",
       " 394,\n",
       " 2,\n",
       " 0,\n",
       " 47,\n",
       " 1,\n",
       " 2,\n",
       " 5903,\n",
       " 79,\n",
       " 32,\n",
       " 5110,\n",
       " 51,\n",
       " 408,\n",
       " 60,\n",
       " 29,\n",
       " 38,\n",
       " 5812,\n",
       " 873,\n",
       " 4,\n",
       " 440,\n",
       " 538,\n",
       " 6202,\n",
       " 60,\n",
       " 38,\n",
       " 50,\n",
       " 2,\n",
       " 11,\n",
       " 471,\n",
       " 51,\n",
       " 399,\n",
       " 988,\n",
       " 77,\n",
       " 9888,\n",
       " 5,\n",
       " 25,\n",
       " 1993,\n",
       " 7,\n",
       " 2044,\n",
       " 96,\n",
       " 41,\n",
       " 248,\n",
       " 118,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2790,\n",
       " 450,\n",
       " 805,\n",
       " 41,\n",
       " 202,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 553,\n",
       " 41,\n",
       " 4998,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 144,\n",
       " 149,\n",
       " 1413,\n",
       " 8361,\n",
       " 2,\n",
       " 29,\n",
       " 19,\n",
       " 3866,\n",
       " 1439,\n",
       " 3228,\n",
       " 542,\n",
       " 5,\n",
       " 184,\n",
       " 2719,\n",
       " 149,\n",
       " 88,\n",
       " 19,\n",
       " 1684,\n",
       " 1973,\n",
       " 8,\n",
       " 197,\n",
       " 85,\n",
       " 753,\n",
       " 5,\n",
       " 1195,\n",
       " 51,\n",
       " 392,\n",
       " 2,\n",
       " 67,\n",
       " 77,\n",
       " 24,\n",
       " 4124,\n",
       " 7,\n",
       " 1684,\n",
       " 1973,\n",
       " 0,\n",
       " 60,\n",
       " 24,\n",
       " 77,\n",
       " 18,\n",
       " 3,\n",
       " 118,\n",
       " 3,\n",
       " 3,\n",
       " 120,\n",
       " 688,\n",
       " 248,\n",
       " 3,\n",
       " 3,\n",
       " 230,\n",
       " 18,\n",
       " 3,\n",
       " 2,\n",
       " 399,\n",
       " 1,\n",
       " 76,\n",
       " 4,\n",
       " 5575,\n",
       " 4,\n",
       " 4625,\n",
       " 7733,\n",
       " 455,\n",
       " 41,\n",
       " 26,\n",
       " 2363,\n",
       " 17,\n",
       " 224,\n",
       " 67,\n",
       " 112,\n",
       " 616,\n",
       " 5,\n",
       " 425,\n",
       " 790,\n",
       " 2,\n",
       " 90,\n",
       " 4,\n",
       " 0,\n",
       " 60,\n",
       " 399,\n",
       " 844,\n",
       " 544,\n",
       " 20,\n",
       " 330,\n",
       " 321,\n",
       " 3623,\n",
       " 210,\n",
       " 5958,\n",
       " 156,\n",
       " 280,\n",
       " 2,\n",
       " 280,\n",
       " 15,\n",
       " 90,\n",
       " 4,\n",
       " 51,\n",
       " 137,\n",
       " 1255,\n",
       " 116,\n",
       " 16,\n",
       " 0,\n",
       " 61,\n",
       " 1051,\n",
       " 3889,\n",
       " 3134,\n",
       " 2,\n",
       " 99,\n",
       " 18,\n",
       " 3,\n",
       " 54,\n",
       " 4,\n",
       " 0,\n",
       " 47,\n",
       " 9,\n",
       " 9268,\n",
       " 2320,\n",
       " 453,\n",
       " 841,\n",
       " 19,\n",
       " 0,\n",
       " 833,\n",
       " 3,\n",
       " 288,\n",
       " 315,\n",
       " 50,\n",
       " 1621,\n",
       " 3,\n",
       " 394,\n",
       " 2438,\n",
       " 5,\n",
       " 384,\n",
       " 6,\n",
       " 1,\n",
       " 470,\n",
       " 7,\n",
       " 0,\n",
       " 447,\n",
       " 2614,\n",
       " 2,\n",
       " 124,\n",
       " 31,\n",
       " 331,\n",
       " 1198,\n",
       " 17,\n",
       " 0,\n",
       " 129,\n",
       " 146,\n",
       " 8,\n",
       " 0,\n",
       " 482,\n",
       " 3201,\n",
       " 111,\n",
       " 77,\n",
       " 24,\n",
       " 2152,\n",
       " 4124,\n",
       " 7,\n",
       " 482,\n",
       " 2,\n",
       " 78,\n",
       " 0,\n",
       " 77,\n",
       " 2671,\n",
       " 7,\n",
       " 0,\n",
       " 833,\n",
       " 3,\n",
       " 6160,\n",
       " 7,\n",
       " 482,\n",
       " 5575,\n",
       " 4,\n",
       " 399,\n",
       " 608,\n",
       " 5,\n",
       " 775,\n",
       " 149,\n",
       " 873,\n",
       " 16,\n",
       " 0,\n",
       " 129,\n",
       " 146,\n",
       " 8,\n",
       " 2198,\n",
       " 608,\n",
       " 5,\n",
       " 1,\n",
       " 112,\n",
       " 118,\n",
       " 2,\n",
       " 19,\n",
       " 6,\n",
       " 410,\n",
       " 0,\n",
       " 3288,\n",
       " 178,\n",
       " 0,\n",
       " 288,\n",
       " 8,\n",
       " 60,\n",
       " 190,\n",
       " 1,\n",
       " 3148,\n",
       " 2,\n",
       " 349,\n",
       " 0,\n",
       " 1,\n",
       " 4,\n",
       " 974,\n",
       " 288,\n",
       " 0,\n",
       " 6766,\n",
       " 4,\n",
       " 251,\n",
       " 280,\n",
       " 316,\n",
       " 0,\n",
       " 1267,\n",
       " 60,\n",
       " 47,\n",
       " 13,\n",
       " 1654,\n",
       " 97,\n",
       " 280,\n",
       " 46,\n",
       " 4224,\n",
       " 5,\n",
       " 457,\n",
       " 60,\n",
       " 112,\n",
       " 1185,\n",
       " 16,\n",
       " 0,\n",
       " 129,\n",
       " 146,\n",
       " 2,\n",
       " 0,\n",
       " 288,\n",
       " 2671,\n",
       " 24,\n",
       " 113,\n",
       " 1,\n",
       " 17,\n",
       " 129,\n",
       " 146,\n",
       " 1022,\n",
       " 280,\n",
       " 2,\n",
       " 14,\n",
       " 1,\n",
       " 607,\n",
       " 52,\n",
       " 15,\n",
       " 54,\n",
       " 137,\n",
       " 2569,\n",
       " 2,\n",
       " 39,\n",
       " 4246,\n",
       " 2853,\n",
       " 4124,\n",
       " 54,\n",
       " 942,\n",
       " 4,\n",
       " 156,\n",
       " 77,\n",
       " 60,\n",
       " 216,\n",
       " 975,\n",
       " 10,\n",
       " 1101,\n",
       " 4312,\n",
       " 0,\n",
       " 288,\n",
       " 8,\n",
       " 60,\n",
       " 190,\n",
       " 8,\n",
       " 30,\n",
       " 58,\n",
       " 3065,\n",
       " 17,\n",
       " 57,\n",
       " 11,\n",
       " 0,\n",
       " 47,\n",
       " 9,\n",
       " 129,\n",
       " 2042,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 974,\n",
       " 975,\n",
       " 184,\n",
       " 156,\n",
       " 280,\n",
       " 182,\n",
       " 36,\n",
       " 184,\n",
       " 129,\n",
       " 3795,\n",
       " 4,\n",
       " 149,\n",
       " 8,\n",
       " 1195,\n",
       " 0,\n",
       " 218,\n",
       " 7,\n",
       " 288,\n",
       " 5,\n",
       " 4938,\n",
       " 7,\n",
       " 6,\n",
       " 115,\n",
       " 1568,\n",
       " 2,\n",
       " 67,\n",
       " 0,\n",
       " 763,\n",
       " 529,\n",
       " 544,\n",
       " 148,\n",
       " 14,\n",
       " 1,\n",
       " 538,\n",
       " 1794,\n",
       " 64,\n",
       " 50,\n",
       " 11,\n",
       " 0,\n",
       " 3029,\n",
       " 15,\n",
       " 6,\n",
       " 1275,\n",
       " 276,\n",
       " 18,\n",
       " 54,\n",
       " 4,\n",
       " 0,\n",
       " 413,\n",
       " 2875,\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
      "Epoch: 1 Learning rate: 1.000\n",
      "0.004 perplexity:1.000 speed: 11516 wps\n",
      "0.104 perplexity:1.000 speed: 31775 wps\n",
      "0.204 perplexity:1.000 speed: 32989 wps\n",
      "0.304 perplexity:1.000 speed: 33488 wps\n",
      "0.404 perplexity:1.000 speed: 33744 wps\n",
      "0.504 perplexity:1.000 speed: 33889 wps\n",
      "0.604 perplexity:1.000 speed: 34009 wps\n",
      "0.703 perplexity:1.000 speed: 34088 wps\n",
      "0.803 perplexity:1.000 speed: 34140 wps\n",
      "0.903 perplexity:1.000 speed: 34185 wps\n",
      "Epoch: 1 Train Perplexity: 1.000\n",
      "Epoch: 1 Valid Perplexity: 1.000\n",
      "Epoch: 2 Learning rate: 1.000\n",
      "0.004 perplexity:1.000 speed: 34678 wps\n",
      "0.104 perplexity:1.000 speed: 34634 wps\n",
      "0.204 perplexity:1.000 speed: 34644 wps\n",
      "0.304 perplexity:1.000 speed: 34668 wps\n",
      "0.404 perplexity:1.000 speed: 34646 wps\n",
      "0.504 perplexity:1.000 speed: 34649 wps\n",
      "0.604 perplexity:1.000 speed: 34659 wps\n",
      "0.703 perplexity:1.000 speed: 34641 wps\n",
      "0.803 perplexity:1.000 speed: 34634 wps\n",
      "0.903 perplexity:1.000 speed: 34636 wps\n",
      "Epoch: 2 Train Perplexity: 1.000\n",
      "Epoch: 2 Valid Perplexity: 1.000\n",
      "Epoch: 3 Learning rate: 1.000\n",
      "0.004 perplexity:1.000 speed: 34733 wps\n",
      "0.104 perplexity:1.000 speed: 34615 wps\n",
      "0.204 perplexity:1.000 speed: 34540 wps\n",
      "0.304 perplexity:1.000 speed: 34582 wps\n",
      "0.404 perplexity:1.000 speed: 34597 wps\n",
      "0.504 perplexity:1.000 speed: 34559 wps\n",
      "0.604 perplexity:1.000 speed: 34579 wps\n",
      "0.703 perplexity:1.000 speed: 34594 wps\n",
      "0.803 perplexity:1.000 speed: 34621 wps\n",
      "0.903 perplexity:1.000 speed: 34637 wps\n",
      "Epoch: 3 Train Perplexity: 1.000\n",
      "Epoch: 3 Valid Perplexity: 1.000\n",
      "Epoch: 4 Learning rate: 1.000\n",
      "0.004 perplexity:1.000 speed: 34932 wps\n",
      "0.104 perplexity:1.000 speed: 34754 wps\n",
      "0.204 perplexity:1.000 speed: 34693 wps\n",
      "0.304 perplexity:1.000 speed: 34627 wps\n",
      "0.404 perplexity:1.000 speed: 34663 wps\n",
      "0.504 perplexity:1.000 speed: 34685 wps\n",
      "0.604 perplexity:1.000 speed: 34690 wps\n",
      "0.703 perplexity:1.000 speed: 34698 wps\n",
      "0.803 perplexity:1.000 speed: 34704 wps\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)     #\n",
    "    \n",
    "    with tf.name_scope(\"Train\"):\n",
    "        train_input = PTBInput(config=config,data=train_data,name=\"TrainInput\")\n",
    "        with tf.variable_scope(\"Model\",reuse = None, initializer=initializer):\n",
    "            m = PTBModel(is_training=True,config=config,input_=train_input)\n",
    "    \n",
    "    with tf.name_scope(\"Valid\"):\n",
    "        valid_input = PTBInput(config=config,data=valid_data,name=\"ValidInput\")\n",
    "        with tf.variable_scope(\"Model\",reuse = True, initializer=initializer):\n",
    "            mvalid = PTBModel(is_training=False,config=config,input_=valid_input)\n",
    "            \n",
    "    with tf.name_scope(\"Test\"):\n",
    "        test_input = PTBInput(config=config,data=test_data,name=\"TestInput\")\n",
    "        with tf.variable_scope(\"Model\",reuse = True, initializer=initializer):\n",
    "            mtest = PTBModel(is_training=False,config=config,input_=test_input)\n",
    "    sv = tf.train.Supervisor()\n",
    "    with sv.managed_session() as session:\n",
    "        for i in range(config.max_max_epoch):\n",
    "            lr_decay = config.lr_decay ** max(i+1-config.max_epoch,0.0)\n",
    "            m.assign_lr(session,config.learning_rate * lr_decay)\n",
    "            \n",
    "            print(\"Epoch: %d Learning rate: %.3f\" % (i+1,session.run(m.lr)))\n",
    "            \n",
    "            train_perplexity = run_epoch(session,m,eval_op=m.train_op,verbose=True)\n",
    "            print(\"Epoch: %d Train Perplexity: %.3f\" % (i+1,train_perplexity))\n",
    "            \n",
    "            valid_perplexity = run_epoch(session,mvalid)\n",
    "            print(\"Epoch: %d Valid Perplexity: %.3f\" % (i+1,valid_perplexity))\n",
    "            \n",
    "        test_perplexity = run_epoch(session,mtest)\n",
    "        print(\"Test Perplexity: %.3f\" % test_perplexity)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
